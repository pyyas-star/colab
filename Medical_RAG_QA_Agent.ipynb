{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain-Specific Medical LLM Q&A Agent (RAG + LLM)\n",
    "\n",
    "**A Retrieval-Augmented Generation System for Safe, Evidence-Based Medical Question Answering**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "This notebook implements a **Domain-Specific Medical Q&A Agent** using **Retrieval-Augmented Generation (RAG)** combined with a powerful **LLM**. The system is designed to answer medical questions using **only trusted, publicly available medical guidelines** (e.g., WHO, CDC), making it suitable for health-tech applications.\n",
    "\n",
    "### Key Features:\n",
    "- ‚úÖ **RAG Architecture**: Combines retrieval with LLM generation for accurate, evidence-based answers\n",
    "- ‚úÖ **Medical Domain Focus**: Uses only trusted medical guidelines (WHO, CDC)\n",
    "- ‚úÖ **Safety First**: Built-in safety checks and medical disclaimers\n",
    "- ‚úÖ **Citation Support**: All answers include source citations\n",
    "- ‚úÖ **Production-Ready**: Modular, well-documented, and deployment-ready\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Overview\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances LLM responses by:\n",
    "\n",
    "1. **Retrieving** relevant documents from a knowledge base\n",
    "2. **Augmenting** the LLM prompt with retrieved context\n",
    "3. **Generating** answers based on the provided context\n",
    "\n",
    "### Why RAG for Medical Applications?\n",
    "\n",
    "- **Reduces Hallucinations**: LLMs alone can generate plausible but incorrect medical information\n",
    "- **Evidence-Based**: Answers are grounded in actual medical guidelines\n",
    "- **Up-to-Date**: Knowledge base can be updated without retraining the model\n",
    "- **Transparency**: Citations show where information comes from\n",
    "- **Safety**: Built-in validation prevents harmful responses\n",
    "\n",
    "### Pipeline Flow:\n",
    "\n",
    "```\n",
    "Medical Documents ‚Üí Chunking ‚Üí Embeddings ‚Üí Vector Store\n",
    "                                                     ‚Üì\n",
    "User Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Retrieve Top-K Docs\n",
    "                                                     ‚Üì\n",
    "Retrieved Context + Query ‚Üí LLM Prompt ‚Üí Generated Answer + Citations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Medical Disclaimer\n",
    "\n",
    "**IMPORTANT**: This system provides general medical information for educational purposes only. It is **NOT** a substitute for professional medical advice, diagnosis, or treatment. Always consult qualified healthcare providers for personal medical concerns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Section 1: Setup & Installation\n",
    "\n",
    "### Google Colab Setup\n",
    "\n",
    "If running in Google Colab, enable GPU for faster processing:\n",
    "- Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
    "\n",
    "### Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q torch transformers sentence-transformers faiss-cpu accelerate numpy pandas pyyaml requests tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Using CPU (slower but works)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path.cwd()))\n",
    "\n",
    "# Import utility modules\n",
    "from utils import (\n",
    "    load_data, clean_text, structure_documents,\n",
    "    MedicalChunker, extract_metadata,\n",
    "    EmbeddingGenerator,\n",
    "    VectorStore, build_vector_store,\n",
    "    RetrievalEngine,\n",
    "    RAGPipeline,\n",
    "    RAGEvaluator,\n",
    "    MedicalSafetyChecker\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Section 2: Data Ingestion\n",
    "\n",
    "### Why Preprocessing Matters for Medical Accuracy\n",
    "\n",
    "Medical documents often contain:\n",
    "- Formatting inconsistencies\n",
    "- Special characters and symbols\n",
    "- Multiple languages or translations\n",
    "- Structured sections (symptoms, treatment, prevention)\n",
    "\n",
    "Proper preprocessing ensures:\n",
    "- **Better Retrieval**: Clean text improves embedding quality\n",
    "- **Reduced Noise**: Removes irrelevant formatting\n",
    "- **Consistent Structure**: Makes chunking more effective\n",
    "- **Metadata Extraction**: Identifies diseases, sections, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path(\"config/config.yaml\")\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "else:\n",
    "    # Default configuration\n",
    "    config = {\n",
    "        'chunking': {'chunk_size': 1000, 'chunk_overlap': 200},\n",
    "        'embedding': {'model_name': 'sentence-transformers/all-MiniLM-L6-v2', 'batch_size': 32},\n",
    "        'retrieval': {'top_k': 5},\n",
    "        'llm': {'model_name': 'google/flan-t5-base', 'max_length': 512}\n",
    "    }\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load medical documents\n",
    "data_file = Path(\"data/sample_medical_guidelines.txt\")\n",
    "\n",
    "if data_file.exists():\n",
    "    raw_documents = load_data(data_file, source_type=\"file\")\n",
    "    print(f\"‚úÖ Loaded {len(raw_documents)} document(s)\")\n",
    "    print(f\"Total characters: {sum(len(doc) for doc in raw_documents):,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Sample data file not found. Using empty list.\")\n",
    "    raw_documents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess documents\n",
    "cleaned_documents = []\n",
    "for doc in raw_documents:\n",
    "    cleaned = clean_text(doc)\n",
    "    cleaned_documents.append(cleaned)\n",
    "\n",
    "print(f\"‚úÖ Cleaned {len(cleaned_documents)} document(s)\")\n",
    "print(f\"\\nSample cleaned text (first 500 chars):\")\n",
    "if cleaned_documents:\n",
    "    print(cleaned_documents[0][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure documents with metadata\n",
    "structured_docs = structure_documents(cleaned_documents)\n",
    "\n",
    "print(f\"‚úÖ Structured {len(structured_docs)} document(s)\")\n",
    "print(f\"\\nSample document structure:\")\n",
    "if structured_docs:\n",
    "    sample = structured_docs[0]\n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"Title: {sample.get('title', 'N/A')}\")\n",
    "    print(f\"Source: {sample.get('source', 'N/A')}\")\n",
    "    print(f\"Text length: {len(sample['text'])} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Section 3: Text Chunking & Preprocessing\n",
    "\n",
    "### Why Chunk Size Affects Retrieval Accuracy\n",
    "\n",
    "**Chunk Size Trade-offs:**\n",
    "- **Too Small**: Loses context, fragments information\n",
    "- **Too Large**: Includes irrelevant information, reduces precision\n",
    "- **Optimal**: 500-1000 characters balances context and precision\n",
    "\n",
    "**Overlap Importance:**\n",
    "- Prevents information loss at chunk boundaries\n",
    "- Ensures continuity for multi-chunk answers\n",
    "- Typical overlap: 10-20% of chunk size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chunker\n",
    "chunk_size = config.get('chunking', {}).get('chunk_size', 1000)\n",
    "chunk_overlap = config.get('chunking', {}).get('chunk_overlap', 200)\n",
    "\n",
    "chunker = MedicalChunker(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    min_chunk_size=100\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Chunker initialized:\")\n",
    "print(f\"  - Chunk size: {chunk_size} characters\")\n",
    "print(f\"  - Overlap: {chunk_overlap} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "\n",
    "for doc in structured_docs:\n",
    "    # Extract metadata\n",
    "    enhanced_metadata = extract_metadata(doc['text'], doc.get('metadata', {}))\n",
    "    \n",
    "    # Chunk document\n",
    "    chunks = chunker.chunk_text(\n",
    "        doc['text'],\n",
    "        document_id=doc['id'],\n",
    "        metadata=enhanced_metadata\n",
    "    )\n",
    "    \n",
    "    # Convert to dict format\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\n",
    "            'text': chunk.text,\n",
    "            'chunk_id': chunk.chunk_id,\n",
    "            'document_id': chunk.document_id,\n",
    "            'metadata': chunk.metadata\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Created {len(all_chunks)} chunks from {len(structured_docs)} documents\")\n",
    "print(f\"\\nChunk statistics:\")\n",
    "chunk_lengths = [len(c['text']) for c in all_chunks]\n",
    "print(f\"  - Average length: {np.mean(chunk_lengths):.0f} characters\")\n",
    "print(f\"  - Min length: {min(chunk_lengths)} characters\")\n",
    "print(f\"  - Max length: {max(chunk_lengths)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Section 4: Embedding & Vector Store\n",
    "\n",
    "### How Embeddings Represent Medical Concepts\n",
    "\n",
    "**Embeddings** convert text into numerical vectors that capture semantic meaning:\n",
    "- Similar medical concepts have similar vectors\n",
    "- Enables mathematical similarity search\n",
    "- Preserves relationships between terms\n",
    "\n",
    "**Why Vector Search is Required:**\n",
    "- **Speed**: Much faster than keyword search\n",
    "- **Semantic Understanding**: Finds conceptually similar content\n",
    "- **Scalability**: Handles large knowledge bases efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding generator\n",
    "model_name = config.get('embedding', {}).get('model_name', 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "batch_size = config.get('embedding', {}).get('batch_size', 32)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "embedding_generator = EmbeddingGenerator(\n",
    "    model_name=model_name,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Embedding generator initialized\")\n",
    "print(f\"  - Model: {model_name}\")\n",
    "print(f\"  - Embedding dimension: {embedding_generator.embedding_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
    "\n",
    "print(f\"Generating embeddings for {len(chunk_texts)} chunks...\")\n",
    "embeddings = embedding_generator.generate_embeddings(\n",
    "    chunk_texts,\n",
    "    batch_size=batch_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated embeddings\")\n",
    "print(f\"  - Shape: {embeddings.shape}\")\n",
    "print(f\"  - Data type: {embeddings.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector store\n",
    "vector_store_path = Path(\"data/vector_store\")\n",
    "\n",
    "vector_store = build_vector_store(\n",
    "    embeddings=embeddings,\n",
    "    chunks=all_chunks,\n",
    "    save_path=vector_store_path\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store built and saved\")\n",
    "print(f\"  - Total vectors: {vector_store.get_stats()['total_vectors']}\")\n",
    "print(f\"  - Saved to: {vector_store_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Section 5: Retrieval Pipeline\n",
    "\n",
    "### Why Retrieval Reduces Hallucinations\n",
    "\n",
    "**Without Retrieval (LLM-only):**\n",
    "- LLM relies on training data (may be outdated)\n",
    "- Can generate plausible but incorrect information\n",
    "- No way to verify sources\n",
    "\n",
    "**With Retrieval (RAG):**\n",
    "- Answers grounded in actual documents\n",
    "- Can cite specific sources\n",
    "- Knowledge base can be updated independently\n",
    "- Reduces fabrication of medical facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrieval engine\n",
    "top_k = config.get('retrieval', {}).get('top_k', 5)\n",
    "\n",
    "retrieval_engine = RetrievalEngine(\n",
    "    vector_store=vector_store,\n",
    "    embedding_generator=embedding_generator,\n",
    "    top_k=top_k,\n",
    "    score_threshold=0.0\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Retrieval engine initialized\")\n",
    "print(f\"  - Top-K: {top_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Section 6: LLM Answer Generation (RAG)\n",
    "\n",
    "### How the LLM Uses Context from Guidelines\n",
    "\n",
    "**Prompt Construction:**\n",
    "1. Retrieved documents are formatted as context\n",
    "2. User query is added\n",
    "3. Instructions guide the LLM to use only the context\n",
    "4. Citation requirements are specified\n",
    "\n",
    "**Why Prompting is Critical:**\n",
    "- Prevents LLM from using outdated training data\n",
    "- Ensures answers are evidence-based\n",
    "- Enforces citation requirements\n",
    "- Maintains medical safety standards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG pipeline\n",
    "llm_model_name = config.get('llm', {}).get('model_name', 'google/flan-t5-base')\n",
    "use_api = config.get('llm', {}).get('use_api', False)\n",
    "api_key = config.get('llm', {}).get('api_key')\n",
    "\n",
    "print(f\"Initializing RAG Pipeline...\")\n",
    "print(f\"  - LLM Model: {llm_model_name}\")\n",
    "print(f\"  - Use API: {use_api}\")\n",
    "\n",
    "rag_pipeline = RAGPipeline(\n",
    "    retrieval_engine=retrieval_engine,\n",
    "    llm_model_name=llm_model_name,\n",
    "    use_api=use_api,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ RAG Pipeline initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete RAG pipeline\n",
    "test_query = \"What are the symptoms of malaria?\"\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Generating answer...\\n\")\n",
    "\n",
    "result = rag_pipeline.generate_answer(\n",
    "    query=test_query,\n",
    "    top_k=5,\n",
    "    include_citations=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(result['answer'])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nSources: {len(result['sources'])}\")\n",
    "print(f\"Valid: {result['is_valid']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Section 7: Testing & Evaluation\n",
    "\n",
    "### Real-World Performance Demonstration\n",
    "\n",
    "This section demonstrates:\n",
    "- Answer quality on medical questions\n",
    "- Citation accuracy\n",
    "- Comparison with baseline (LLM-only)\n",
    "- Evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test questions\n",
    "test_questions_file = Path(\"data/test_questions.json\")\n",
    "\n",
    "if test_questions_file.exists():\n",
    "    with open(test_questions_file, 'r') as f:\n",
    "        test_questions = json.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(test_questions)} test questions\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Test questions file not found. Using sample questions.\")\n",
    "    test_questions = [\n",
    "        {\"query\": \"What are the symptoms of malaria?\"},\n",
    "        {\"query\": \"How is tuberculosis treated?\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on multiple questions\n",
    "print(\"Testing RAG System on Medical Questions:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(test_questions[:3], 1):  # Test first 3\n",
    "    query = test_case['query']\n",
    "    print(f\"\\n[{i}] Query: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = rag_pipeline.generate_answer(query, top_k=3)\n",
    "    \n",
    "    print(f\"Answer: {result['answer'][:300]}...\")\n",
    "    print(f\"\\nSources: {len(result['sources'])}\")\n",
    "    if result.get('safety_warning'):\n",
    "        print(f\"Safety Warning: {result['safety_warning']}\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Section 8: Making it App-Ready (Bonus)\n",
    "\n",
    "### Deployment Possibilities\n",
    "\n",
    "This notebook structure can easily be converted into:\n",
    "\n",
    "1. **FastAPI Endpoint**: REST API for medical Q&A\n",
    "2. **Streamlit App**: Interactive web interface\n",
    "3. **Chatbot Interface**: Conversational medical assistant\n",
    "4. **Mobile App Backend**: API for mobile health apps\n",
    "\n",
    "### Next Steps for Production\n",
    "\n",
    "1. **Add Authentication**: Secure API access\n",
    "2. **Rate Limiting**: Prevent abuse\n",
    "3. **Logging**: Track usage and errors\n",
    "4. **Monitoring**: Health checks and metrics\n",
    "5. **Caching**: Cache common queries\n",
    "6. **Database**: Store query history\n",
    "7. **CI/CD**: Automated testing and deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary & Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "‚úÖ **Complete RAG Pipeline**: From data ingestion to answer generation\n",
    "‚úÖ **Medical Domain Focus**: Safety checks and evidence-based answers\n",
    "‚úÖ **Production-Ready Code**: Modular, documented, and scalable\n",
    "‚úÖ **Evaluation Framework**: Metrics and benchmarking tools\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **RAG Architecture**: How retrieval enhances LLM responses\n",
    "2. **Medical Safety**: Importance of validation and disclaimers\n",
    "3. **Vector Search**: Efficient semantic similarity search\n",
    "4. **Prompt Engineering**: Critical for medical accuracy\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "- **Larger Knowledge Base**: Add more medical guidelines\n",
    "- **Better Embeddings**: Fine-tune on medical text\n",
    "- **Reranking**: Improve retrieval quality\n",
    "- **Multi-turn Conversations**: Context-aware follow-ups\n",
    "- **Multilingual Support**: Answer in multiple languages\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for using the Medical RAG Q&A Agent!**\n",
    "\n",
    "Remember: This system is for educational purposes only. Always consult healthcare professionals for medical advice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
