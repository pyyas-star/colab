{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLLB Amharic-English Translation Evaluation\n",
        "\n",
        "This notebook evaluates three NLLB models (600M, 1.3B, and 3.3B) for bidirectional Amharic-English translation.\n",
        "\n",
        "**Models to evaluate:**\n",
        "- `facebook/nllb-200-distilled-600M` (600M parameters)\n",
        "- `facebook/nllb-200-1.3B` (1.3B parameters)\n",
        "- `facebook/nllb-200-3.3B` (3.3B parameters)\n",
        "\n",
        "**Evaluation includes:**\n",
        "- Simple sentences (greetings, questions)\n",
        "- Complex sentences (multi-clause, conditionals)\n",
        "- Domain-specific text (medical, technical, financial)\n",
        "- Paragraphs (long-form text)\n",
        "- Bidirectional translation (Amharic ↔ English)\n",
        "\n",
        "**Metrics:**\n",
        "- BLEU Score\n",
        "- chrF Score\n",
        "- Translation time per sentence\n",
        "- Overall performance comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch transformers sentencepiece sacrebleu pandas numpy tqdm -q\n",
        "%pip install accelerate bitsandbytes -q  # For memory optimization\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from sacrebleu import BLEU, CHRF\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define comprehensive test dataset\n",
        "# This includes all original test pairs plus additional extensive examples\n",
        "\n",
        "EXTENSIVE_TEST_DATASET = {\n",
        "    \"test_pairs\": [\n",
        "        # === DAILY CONVERSATION ===\n",
        "        {\"id\": 1, \"category\": \"daily_conversation\", \"amharic\": \"እንዴት ነህ?\", \"english\": \"How are you?\"},\n",
        "        {\"id\": 2, \"category\": \"daily_conversation\", \"amharic\": \"ስምዎ ማን ነው?\", \"english\": \"What is your name?\"},\n",
        "        {\"id\": 3, \"category\": \"daily_conversation\", \"amharic\": \"ኢትዮጵያ ጥሩ ናት።\", \"english\": \"Ethiopia is good.\"},\n",
        "        {\"id\": 4, \"category\": \"daily_conversation\", \"amharic\": \"ዛሬ ምን እየሰራን ነው?\", \"english\": \"What are we doing today?\"},\n",
        "        {\"id\": 5, \"category\": \"daily_conversation\", \"amharic\": \"እባክህ ይሄንን አስተርጎምልኝ።\", \"english\": \"Please translate this for me.\"},\n",
        "        {\"id\": 6, \"category\": \"daily_conversation\", \"amharic\": \"እኔ አሚን ነኝ።\", \"english\": \"I am Amin.\"},\n",
        "        {\"id\": 7, \"category\": \"daily_conversation\", \"amharic\": \"በጣም ደስ ይለኛል።\", \"english\": \"I am very happy.\"},\n",
        "        {\"id\": 8, \"category\": \"daily_conversation\", \"amharic\": \"ምን እየሰራክ ነው?\", \"english\": \"What are you doing?\"},\n",
        "        \n",
        "        # === NEWS/SOCIAL ===\n",
        "        {\"id\": 9, \"category\": \"news_social\", \"amharic\": \"የኢትዮጵያ መንግሥት አዲስ የጤና ፕሮግራሞችን አስጀመረ።\", \"english\": \"The Ethiopian government launched new health programs.\"},\n",
        "        {\"id\": 10, \"category\": \"news_social\", \"amharic\": \"በአዲስ አበባ የተካሄደው ስብሰባ በተሳካ ሁኔታ ተጠናቋል።\", \"english\": \"The meeting held in Addis Ababa was completed successfully.\"},\n",
        "        {\"id\": 11, \"category\": \"news_social\", \"amharic\": \"የትምህርት ሚኒስቴር የአዲስ ትምህርት ሥርዓት አስተዋወቀ።\", \"english\": \"The Ministry of Education announced a new education system.\"},\n",
        "        {\"id\": 12, \"category\": \"news_social\", \"amharic\": \"የኢትዮጵያ አየር መንገድ አዲስ አውሮፕላኖችን ገዝቷል።\", \"english\": \"Ethiopian Airlines bought new airplanes.\"},\n",
        "        \n",
        "        # === COMPLEX SENTENCES ===\n",
        "        {\"id\": 13, \"category\": \"complex\", \"amharic\": \"አንድ ሰው ስለራሱ ማሰብ እንዳይችል እና ስለሌሎች ሰዎች እንዲያስብ የሚያስተምረው ትምህርት አለ።\", \"english\": \"There is a lesson that teaches a person not to think about himself and to think about other people.\"},\n",
        "        {\"id\": 14, \"category\": \"complex\", \"amharic\": \"በኢትዮጵያ ውስጥ የሚኖሩ የተለያዩ የብሔረሰቦች የራሳቸውን ቋንቋ እና ባህላቸውን ለመጠበቅ መብት አላቸው።\", \"english\": \"Different ethnic groups living in Ethiopia have the right to preserve their own language and culture.\"},\n",
        "        {\"id\": 15, \"category\": \"complex\", \"amharic\": \"አንድ ሰው የተወሰነ ነገር ለማድረግ አስቸጋሪ ከሆነ በተለያዩ መንገዶች መሞከር አለበት።\", \"english\": \"When it is difficult for a person to do something specific, they should try different ways.\"},\n",
        "        \n",
        "        # === DOMAIN-SPECIFIC ===\n",
        "        {\"id\": 16, \"category\": \"domain_specific\", \"amharic\": \"የኮምፒዩተር ፕሮግራሚንግ የሚያስተምረው ትምህርት በአሁኑ ጊዜ በጣም አስፈላጊ ነው።\", \"english\": \"The lesson that teaches computer programming is very important now.\"},\n",
        "        {\"id\": 17, \"category\": \"domain_specific\", \"amharic\": \"የተፈጥሮ ሀብቶችን ለመጠበቅ እና ለመጠቀም አዲስ ቴክኖሎጂዎች አስፈላጊ ናቸው።\", \"english\": \"New technologies are necessary to preserve and use natural resources.\"},\n",
        "        {\"id\": 18, \"category\": \"domain_specific\", \"amharic\": \"የባንክ ስርዓት አሁን ከወደፊቱ የኢኮኖሚ እድገት አስፈላጊ ነው።\", \"english\": \"The banking system is important for future economic growth now.\"},\n",
        "        {\"id\": 19, \"category\": \"domain_specific\", \"amharic\": \"የሕክምና ሙያዎች ለሰዎች ጤና አስፈላጊ ናቸው።\", \"english\": \"Medical professions are important for people's health.\"},\n",
        "        {\"id\": 20, \"category\": \"domain_specific\", \"amharic\": \"የአርቲፊሻል ኢንተሊጀንስ ቴክኖሎጂ በፍጥነት እያደገ ነው።\", \"english\": \"Artificial intelligence technology is growing rapidly.\"},\n",
        "        \n",
        "        # === PARAGRAPHS (Long-form text) ===\n",
        "        {\"id\": 21, \"category\": \"paragraph\", \"amharic\": \"ኢትዮጵያ በአፍሪካ ምሥራቅ ውስጥ የምትገኝ ሀገር ናት። ይህች ሀገር ከጥንት ጀምሮ የተለያዩ ባህሎችን እና ቋንቋዎችን አላት። የኢትዮጵያ ህዝብ በጣም ሰላማዊ እና ሞገሳም ነው።\", \"english\": \"Ethiopia is a country located in East Africa. This country has various cultures and languages since ancient times. The Ethiopian people are very peaceful and hospitable.\"},\n",
        "        \n",
        "        {\"id\": 22, \"category\": \"paragraph\", \"amharic\": \"የኮምፒዩተር ሳይንስ በዘመናዊው ዓለም ውስጥ በጣም አስፈላጊ የሆነ የመማሪያ ዘርፍ ነው። ብዙ የኮሌጆች ተማሪዎች ይህንን ሙያ ይመርጣሉ ምክንያቱም ከፍተኛ የስራ እድሎች ስላሉት ነው። የአይቲ ኢንዱስትሪ በፍጥነት እያደገ ነው።\", \"english\": \"Computer science is a very important field of study in the modern world. Many college students choose this profession because it has high job opportunities. The IT industry is growing rapidly.\"},\n",
        "        \n",
        "        {\"id\": 23, \"category\": \"paragraph\", \"amharic\": \"የጤና አጠባበቅ ለሁሉም ሰው አስፈላጊ ነው። ለጤናማ ሕይወት መኖር ምግብን በትክክል መመገብ እና የሰውነት መለማለድ አስፈላጊ ናቸው። እንዲሁም ሙሉ እንቅልፍ መቀበል እና የጤና ክትትል መስራት አስፈላጊ ነው።\", \"english\": \"Health care is important for everyone. To live a healthy life, eating food properly and exercising the body are necessary. Also, getting full sleep and having health checkups are important.\"},\n",
        "        \n",
        "        {\"id\": 24, \"category\": \"paragraph\", \"amharic\": \"የትምህርት ሥርዓት ለሀገር እድገት አስፈላጊ ነው። መንግሥት ለሁሉም ልጆች የትምህርት እድል ለመስጠት እየሰራ ነው። በአሁኑ ጊዜ የአዲስ ቴክኖሎጂ በትምህርት ውስጥ የሚጠቀም ሲሆን ይህ የትምህርት ሥርዓትን ያሻሽላል።\", \"english\": \"The education system is important for national development. The government is working to provide educational opportunities for all children. Currently, new technology is being used in education, and this improves the education system.\"},\n",
        "        \n",
        "        {\"id\": 25, \"category\": \"paragraph\", \"amharic\": \"የኢኮኖሚ እድገት ለሀገር እድገት አስፈላጊ ነው። የንግድ እና የኢንዱስትሪ ኢንቬስትመንቶች ኢኮኖሚውን እያሻሻሉ ነው። መንግሥት የኢንቨስትመንት አካባቢዎችን ለመፍጠር እየሰራ ነው። ይህ የስራ እድሎችን ይፈጥራል እና የሕዝቡን የሕይወት ደረጃ ያሻሽላል።\", \"english\": \"Economic growth is important for national development. Business and industrial investments are improving the economy. The government is working to create investment opportunities. This creates job opportunities and improves the standard of living of the people.\"},\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"Total test pairs: {len(EXTENSIVE_TEST_DATASET['test_pairs'])}\")\n",
        "print(f\"Categories: {set(p['category'] for p in EXTENSIVE_TEST_DATASET['test_pairs'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NLLB Translator Class\n",
        "class NLLBTranslator:\n",
        "    \"\"\"NLLB Translation Model Wrapper\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.lang_codes = {\n",
        "            \"amharic\": \"amh_Ethi\",\n",
        "            \"english\": \"eng_Latn\"\n",
        "        }\n",
        "        \n",
        "    def load_model(self):\n",
        "        \"\"\"Load the NLLB model from Hugging Face Hub\"\"\"\n",
        "        print(f\"Loading {self.model_name}...\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
        "        self.model.eval()\n",
        "        \n",
        "        if self.device == \"cuda\":\n",
        "            self.model = self.model.to(self.device)\n",
        "            print(f\"✓ Model loaded on GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        else:\n",
        "            print(\"✓ Model loaded on CPU\")\n",
        "    \n",
        "    def translate_amh_to_eng(self, amharic_text: str) -> str:\n",
        "        \"\"\"Translate Amharic to English\"\"\"\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
        "        \n",
        "        try:\n",
        "            src_lang_code = self.lang_codes[\"amharic\"]\n",
        "            tgt_lang_code = self.lang_codes[\"english\"]\n",
        "            \n",
        "            self.tokenizer.src_lang = src_lang_code\n",
        "            vocab = self.tokenizer.get_vocab()\n",
        "            \n",
        "            tgt_lang_id = vocab.get(tgt_lang_code)\n",
        "            if tgt_lang_id is None:\n",
        "                raise ValueError(f\"Could not find language code token: {tgt_lang_code}\")\n",
        "            \n",
        "            inputs = self.tokenizer(amharic_text, return_tensors=\"pt\")\n",
        "            \n",
        "            if self.device == \"cuda\":\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                generated_tokens = self.model.generate(\n",
        "                    **inputs,\n",
        "                    forced_bos_token_id=tgt_lang_id,\n",
        "                    max_length=512,\n",
        "                    num_beams=5,\n",
        "                    length_penalty=1.0\n",
        "                )\n",
        "            \n",
        "            translation = self.tokenizer.batch_decode(\n",
        "                generated_tokens, \n",
        "                skip_special_tokens=True\n",
        "            )[0]\n",
        "            \n",
        "            return translation\n",
        "        except Exception as e:\n",
        "            return f\"[Error: {str(e)}]\"\n",
        "    \n",
        "    def translate_eng_to_amh(self, english_text: str) -> str:\n",
        "        \"\"\"Translate English to Amharic\"\"\"\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
        "        \n",
        "        try:\n",
        "            src_lang_code = self.lang_codes[\"english\"]\n",
        "            tgt_lang_code = self.lang_codes[\"amharic\"]\n",
        "            \n",
        "            self.tokenizer.src_lang = src_lang_code\n",
        "            vocab = self.tokenizer.get_vocab()\n",
        "            \n",
        "            tgt_lang_id = vocab.get(tgt_lang_code)\n",
        "            if tgt_lang_id is None:\n",
        "                raise ValueError(f\"Could not find language code token: {tgt_lang_code}\")\n",
        "            \n",
        "            inputs = self.tokenizer(english_text, return_tensors=\"pt\")\n",
        "            \n",
        "            if self.device == \"cuda\":\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                generated_tokens = self.model.generate(\n",
        "                    **inputs,\n",
        "                    forced_bos_token_id=tgt_lang_id,\n",
        "                    max_length=512,\n",
        "                    num_beams=5,\n",
        "                    length_penalty=1.0\n",
        "                )\n",
        "            \n",
        "            translation = self.tokenizer.batch_decode(\n",
        "                generated_tokens, \n",
        "                skip_special_tokens=True\n",
        "            )[0]\n",
        "            \n",
        "            return translation\n",
        "        except Exception as e:\n",
        "            return f\"[Error: {str(e)}]\"\n",
        "\n",
        "print(\"✓ Translator class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(translator, test_pairs, direction=\"amh_to_eng\"):\n",
        "    \"\"\"Evaluate translation quality for a given direction\"\"\"\n",
        "    bleu = BLEU()\n",
        "    chrf = CHRF()\n",
        "    \n",
        "    sources = []\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    translation_times = []\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Evaluating {direction.upper().replace('_', ' → ')}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    for pair in tqdm(test_pairs, desc=\"Translating\"):\n",
        "        if direction == \"amh_to_eng\":\n",
        "            source = pair[\"amharic\"]\n",
        "            reference = pair[\"english\"]\n",
        "            start_time = time.time()\n",
        "            translation = translator.translate_amh_to_eng(source)\n",
        "            translation_time = time.time() - start_time\n",
        "        else:  # eng_to_amh\n",
        "            source = pair[\"english\"]\n",
        "            reference = pair[\"amharic\"]\n",
        "            start_time = time.time()\n",
        "            translation = translator.translate_eng_to_amh(source)\n",
        "            translation_time = time.time() - start_time\n",
        "        \n",
        "        sources.append(source)\n",
        "        references.append(reference)\n",
        "        hypotheses.append(translation)\n",
        "        translation_times.append(translation_time)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    bleu_score = bleu.corpus_score(hypotheses, [references]).score\n",
        "    chrf_score = chrf.corpus_score(hypotheses, [references]).score\n",
        "    \n",
        "    results = {\n",
        "        \"direction\": direction,\n",
        "        \"model\": translator.model_name,\n",
        "        \"metrics\": {\n",
        "            \"bleu\": bleu_score,\n",
        "            \"chrf\": chrf_score\n",
        "        },\n",
        "        \"performance\": {\n",
        "            \"total_time\": sum(translation_times),\n",
        "            \"avg_time_per_sentence\": sum(translation_times) / len(translation_times),\n",
        "            \"sentences_per_second\": len(test_pairs) / sum(translation_times)\n",
        "        },\n",
        "        \"translations\": [\n",
        "            {\n",
        "                \"id\": pair[\"id\"],\n",
        "                \"category\": pair[\"category\"],\n",
        "                \"source\": src,\n",
        "                \"reference\": ref,\n",
        "                \"translation\": hyp,\n",
        "                \"time\": t\n",
        "            }\n",
        "            for pair, src, ref, hyp, t in zip(test_pairs, sources, references, hypotheses, translation_times)\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nBLEU Score: {bleu_score:.2f}\")\n",
        "    print(f\"chrF Score: {chrf_score:.2f}\")\n",
        "    print(f\"Avg Time/Sentence: {results['performance']['avg_time_per_sentence']:.2f}s\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ Evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "We'll evaluate all three models sequentially (since running them in parallel would require too much GPU memory).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models to evaluate\n",
        "MODELS = [\n",
        "    \"facebook/nllb-200-distilled-600M\",\n",
        "    \"facebook/nllb-200-1.3B\",\n",
        "    \"facebook/nllb-200-3.3B\"\n",
        "]\n",
        "\n",
        "test_pairs = EXTENSIVE_TEST_DATASET[\"test_pairs\"]\n",
        "all_results = {}\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name in MODELS:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EVALUATING MODEL: {model_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    try:\n",
        "        # Load model\n",
        "        translator = NLLBTranslator(model_name)\n",
        "        translator.load_model()\n",
        "        \n",
        "        # Clear GPU cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        # Evaluate both directions\n",
        "        amh_to_eng_results = evaluate_model(translator, test_pairs, \"amh_to_eng\")\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        eng_to_amh_results = evaluate_model(translator, test_pairs, \"eng_to_amh\")\n",
        "        \n",
        "        all_results[model_name] = {\n",
        "            \"amh_to_eng\": amh_to_eng_results,\n",
        "            \"eng_to_amh\": eng_to_amh_results\n",
        "        }\n",
        "        \n",
        "        # Unload model to free memory\n",
        "        del translator.model\n",
        "        del translator.tokenizer\n",
        "        del translator\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        print(f\"\\n✓ Completed evaluation for {model_name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error evaluating {model_name}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"\\n✓ All evaluations completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n",
        "\n",
        "Generate formatted results for easy copying to Google Docs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive results summary\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Summary table\n",
        "summary_data = []\n",
        "for model_name in MODELS:\n",
        "    if model_name in all_results:\n",
        "        amh_to_eng = all_results[model_name][\"amh_to_eng\"]\n",
        "        eng_to_amh = all_results[model_name][\"eng_to_amh\"]\n",
        "        \n",
        "        summary_data.append({\n",
        "            \"Model\": model_name.replace(\"facebook/nllb-200-\", \"\"),\n",
        "            \"Amh→Eng BLEU\": f\"{amh_to_eng['metrics']['bleu']:.2f}\",\n",
        "            \"Amh→Eng chrF\": f\"{amh_to_eng['metrics']['chrf']:.2f}\",\n",
        "            \"Eng→Amh BLEU\": f\"{eng_to_amh['metrics']['bleu']:.2f}\",\n",
        "            \"Eng→Amh chrF\": f\"{eng_to_amh['metrics']['chrf']:.2f}\",\n",
        "            \"Avg Time/Sent\": f\"{amh_to_eng['performance']['avg_time_per_sentence']:.2f}s\"\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed results formatted for Google Docs\n",
        "def format_results_for_docs(all_results, test_pairs):\n",
        "    \"\"\"Format results in a way that's easy to copy to Google Docs\"\"\"\n",
        "    \n",
        "    output = []\n",
        "    output.append(\"=\"*80)\n",
        "    output.append(\"NLLB AMHARIC-ENGLISH TRANSLATION EVALUATION RESULTS\")\n",
        "    output.append(\"=\"*80)\n",
        "    output.append(\"\")\n",
        "    output.append(f\"Test Dataset: {len(test_pairs)} sentence pairs\")\n",
        "    output.append(f\"Categories: {', '.join(set(p['category'] for p in test_pairs))}\")\n",
        "    output.append(\"\")\n",
        "    \n",
        "    # Summary table\n",
        "    output.append(\"SUMMARY METRICS\")\n",
        "    output.append(\"-\"*80)\n",
        "    output.append(f\"{'Model':<30} {'Amh→Eng BLEU':<15} {'Amh→Eng chrF':<15} {'Eng→Amh BLEU':<15} {'Eng→Amh chrF':<15} {'Avg Time':<10}\")\n",
        "    output.append(\"-\"*80)\n",
        "    \n",
        "    for model_name in MODELS:\n",
        "        if model_name in all_results:\n",
        "            amh_to_eng = all_results[model_name][\"amh_to_eng\"]\n",
        "            eng_to_amh = all_results[model_name][\"eng_to_amh\"]\n",
        "            \n",
        "            model_short = model_name.replace(\"facebook/nllb-200-\", \"\")\n",
        "            output.append(\n",
        "                f\"{model_short:<30} \"\n",
        "                f\"{amh_to_eng['metrics']['bleu']:>14.2f} \"\n",
        "                f\"{amh_to_eng['metrics']['chrf']:>14.2f} \"\n",
        "                f\"{eng_to_amh['metrics']['bleu']:>14.2f} \"\n",
        "                f\"{eng_to_amh['metrics']['chrf']:>14.2f} \"\n",
        "                f\"{amh_to_eng['performance']['avg_time_per_sentence']:>9.2f}s\"\n",
        "            )\n",
        "    \n",
        "    output.append(\"\")\n",
        "    output.append(\"=\"*80)\n",
        "    output.append(\"\")\n",
        "    \n",
        "    # Detailed results for each model\n",
        "    for model_name in MODELS:\n",
        "        if model_name not in all_results:\n",
        "            continue\n",
        "            \n",
        "        model_short = model_name.replace(\"facebook/nllb-200-\", \"\")\n",
        "        output.append(f\"\\n{'='*80}\")\n",
        "        output.append(f\"MODEL: {model_short}\")\n",
        "        output.append(f\"{'='*80}\\n\")\n",
        "        \n",
        "        # Amharic to English\n",
        "        amh_to_eng = all_results[model_name][\"amh_to_eng\"]\n",
        "        output.append(f\"AMHARIC → ENGLISH\")\n",
        "        output.append(f\"BLEU Score: {amh_to_eng['metrics']['bleu']:.2f}\")\n",
        "        output.append(f\"chrF Score: {amh_to_eng['metrics']['chrf']:.2f}\")\n",
        "        output.append(f\"Average Time per Sentence: {amh_to_eng['performance']['avg_time_per_sentence']:.2f}s\")\n",
        "        output.append(\"\")\n",
        "        \n",
        "        # Sample translations by category\n",
        "        for category in [\"daily_conversation\", \"news_social\", \"complex\", \"domain_specific\", \"paragraph\"]:\n",
        "            category_translations = [\n",
        "                t for t in amh_to_eng['translations'] \n",
        "                if next(p for p in test_pairs if p['id'] == t['id'])['category'] == category\n",
        "            ]\n",
        "            \n",
        "            if category_translations:\n",
        "                output.append(f\"\\n{category.upper().replace('_', ' ')} ({len(category_translations)} examples):\")\n",
        "                output.append(\"-\"*80)\n",
        "                for t in category_translations[:5]:  # Show first 5\n",
        "                    output.append(f\"\\nExample {t['id']}:\")\n",
        "                    output.append(f\"  Source (Amharic): {t['source']}\")\n",
        "                    output.append(f\"  Reference (English): {t['reference']}\")\n",
        "                    output.append(f\"  Translation: {t['translation']}\")\n",
        "                    output.append(f\"  Time: {t['time']:.2f}s\")\n",
        "                if len(category_translations) > 5:\n",
        "                    output.append(f\"  ... and {len(category_translations) - 5} more examples\")\n",
        "        \n",
        "        output.append(\"\\n\" + \"-\"*80)\n",
        "        \n",
        "        # English to Amharic\n",
        "        eng_to_amh = all_results[model_name][\"eng_to_amh\"]\n",
        "        output.append(f\"\\nENGLISH → AMHARIC\")\n",
        "        output.append(f\"BLEU Score: {eng_to_amh['metrics']['bleu']:.2f}\")\n",
        "        output.append(f\"chrF Score: {eng_to_amh['metrics']['chrf']:.2f}\")\n",
        "        output.append(f\"Average Time per Sentence: {eng_to_amh['performance']['avg_time_per_sentence']:.2f}s\")\n",
        "        output.append(\"\")\n",
        "        \n",
        "        # Sample translations\n",
        "        for category in [\"daily_conversation\", \"news_social\", \"complex\", \"domain_specific\", \"paragraph\"]:\n",
        "            category_translations = [\n",
        "                t for t in eng_to_amh['translations'] \n",
        "                if next(p for p in test_pairs if p['id'] == t['id'])['category'] == category\n",
        "            ]\n",
        "            \n",
        "            if category_translations:\n",
        "                output.append(f\"\\n{category.upper().replace('_', ' ')} ({len(category_translations)} examples):\")\n",
        "                output.append(\"-\"*80)\n",
        "                for t in category_translations[:5]:  # Show first 5\n",
        "                    output.append(f\"\\nExample {t['id']}:\")\n",
        "                    output.append(f\"  Source (English): {t['source']}\")\n",
        "                    output.append(f\"  Reference (Amharic): {t['reference']}\")\n",
        "                    output.append(f\"  Translation: {t['translation']}\")\n",
        "                    output.append(f\"  Time: {t['time']:.2f}s\")\n",
        "                if len(category_translations) > 5:\n",
        "                    output.append(f\"  ... and {len(category_translations) - 5} more examples\")\n",
        "        \n",
        "        output.append(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "    \n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "# Generate formatted output\n",
        "formatted_output = format_results_for_docs(all_results, test_pairs)\n",
        "print(formatted_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to a text file for easy copying\n",
        "with open(\"evaluation_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(formatted_output)\n",
        "\n",
        "print(\"✓ Results saved to 'evaluation_results.txt'\")\n",
        "print(\"You can download this file and copy its contents to Google Docs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comparison table showing all models side by side\n",
        "comparison_data = []\n",
        "\n",
        "for i, pair in enumerate(test_pairs[:10], 1):  # Show first 10 examples\n",
        "    row = {\n",
        "        \"ID\": i,\n",
        "        \"Category\": pair[\"category\"],\n",
        "        \"Source (Amharic)\": pair[\"amharic\"][:50] + \"...\" if len(pair[\"amharic\"]) > 50 else pair[\"amharic\"],\n",
        "        \"Reference (English)\": pair[\"english\"]\n",
        "    }\n",
        "    \n",
        "    # Add translations from each model\n",
        "    for model_name in MODELS:\n",
        "        if model_name in all_results:\n",
        "            model_short = model_name.replace(\"facebook/nllb-200-\", \"\")\n",
        "            amh_to_eng = all_results[model_name][\"amh_to_eng\"]\n",
        "            translation = next(\n",
        "                (t[\"translation\"] for t in amh_to_eng[\"translations\"] if t[\"id\"] == pair[\"id\"]),\n",
        "                \"N/A\"\n",
        "            )\n",
        "            row[f\"{model_short} Translation\"] = translation[:100] + \"...\" if len(translation) > 100 else translation\n",
        "    \n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SIDE-BY-SIDE COMPARISON (First 10 Examples)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use This Notebook on Google Colab\n",
        "\n",
        "### Step 1: Open the Notebook\n",
        "1. Go to [Google Colab](https://colab.research.google.com/)\n",
        "2. Click \"File\" → \"Open Notebook\"\n",
        "3. Select \"GitHub\" tab\n",
        "4. Enter your GitHub repository URL: `https://github.com/YOUR_USERNAME/YOUR_REPO`\n",
        "5. Select `amh_translation.ipynb`\n",
        "\n",
        "### Step 2: Enable GPU Runtime\n",
        "1. Click \"Runtime\" → \"Change runtime type\"\n",
        "2. Select \"GPU\" from the Hardware accelerator dropdown\n",
        "3. Click \"Save\"\n",
        "\n",
        "### Step 3: Run All Cells\n",
        "1. Click \"Runtime\" → \"Run all\" (or press Ctrl+F9)\n",
        "2. The notebook will:\n",
        "   - Install required packages\n",
        "   - Load and evaluate all three models sequentially\n",
        "   - Generate formatted results\n",
        "\n",
        "### Step 4: Copy Results to Google Docs\n",
        "1. After execution completes, scroll to the results section\n",
        "2. Select the formatted text output\n",
        "3. Copy (Ctrl+C)\n",
        "4. Paste into your Google Doc\n",
        "\n",
        "**Note:** Model download may take 10-30 minutes on first run. Subsequent runs will be faster as models are cached.\n",
        "\n",
        "### Expected Runtime:\n",
        "- **600M model**: ~5-10 minutes\n",
        "- **1.3B model**: ~10-15 minutes  \n",
        "- **3.3B model**: ~15-20 minutes\n",
        "- **Total**: ~30-45 minutes for complete evaluation\n",
        "\n",
        "### Memory Requirements:\n",
        "- Colab free tier provides ~15GB GPU memory (sufficient for all models)\n",
        "- If you run out of memory, models are evaluated sequentially and memory is cleared between models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyyas-star/colab/blob/main/amh_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DgHrq5N3tVv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOMsUdyyeEPPgq8HFzx4TMc",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
